{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d0ddc1-1753-4638-ab77-9ad68a39742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16dba28-9d52-409d-a49d-ff1808927633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip_file_path='spider_data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f003f65e-8ead-4c67-96a8-e224544db2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_dir = 'spider_dataset_extracted'\n",
    "# os.makedirs(extracted_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bff271-8bda-4a51-b146-0dcaf4bb2672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spider_data.zip to spider_dataset_extracted...\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Extracting {zip_file_path} to {extracted_dir}...\")\n",
    "# !unzip -q {zip_file_path} -d {extracted_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a78c4f6-ce86-4afc-a048-7fb47d441845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset # Import Dataset from here\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d214363c-249e-4ed1-b332-93a3e23f65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 166 database schemas from /home/jaggu/Deep_L/spider_dataset_extracted/spider_data/tables.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Configuration ---\n",
    "MODEL_NAME = \"t5-small\" # You can try \"t5-base\" if you have more VRAM\n",
    "TOKENIZER = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- IMPORTANT: Your DATASET_DIR is correct now ---\n",
    "# This should be the path to the 'spider_data' folder\n",
    "DATASET_DIR = \"/home/jaggu/Deep_L/spider_dataset_extracted/spider_data\"\n",
    "\n",
    "# Verify the directory exists\n",
    "if not os.path.isdir(DATASET_DIR):\n",
    "    raise FileNotFoundError(f\"DATASET_DIR '{DATASET_DIR}' does not exist. Please check your path.\")\n",
    "# --- CORRECTED FILENAME CHECK ---\n",
    "if not os.path.exists(os.path.join(DATASET_DIR, \"train_spider.json\")): # <<< CHANGED from train.json\n",
    "    raise FileNotFoundError(f\"train_spider.json not found in '{DATASET_DIR}'. Ensure Spider files are extracted correctly.\")\n",
    "if not os.path.exists(os.path.join(DATASET_DIR, \"dev.json\")):\n",
    "    raise FileNotFoundError(f\"dev.json not found in '{DATASET_DIR}'. Ensure Spider files are extracted correctly.\")\n",
    "if not os.path.exists(os.path.join(DATASET_DIR, \"tables.json\")):\n",
    "    raise FileNotFoundError(f\"tables.json not found in '{DATASET_DIR}'. Ensure Spider files are extracted correctly.\")\n",
    "\n",
    "\n",
    "# --- 2. Load tables.json for Database Schemas ---\n",
    "tables_file_path = os.path.join(DATASET_DIR, 'tables.json')\n",
    "with open(tables_file_path, 'r', encoding='utf-8') as f:\n",
    "    db_schemas = {db['db_id']: db for db in json.load(f)}\n",
    "\n",
    "print(f\"Successfully loaded {len(db_schemas)} database schemas from {tables_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014befaa-95c3-46f9-b35d-01c5692abb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and normalizing Spider train split...\n",
      "Loading and normalizing Spider validation split...\n",
      "\n",
      "Spider Dataset loaded and normalized successfully!\n",
      "{'train': Dataset({\n",
      "    features: ['question', 'query', 'db_id'],\n",
      "    num_rows: 7000\n",
      "}), 'validation': Dataset({\n",
      "    features: ['question', 'query', 'db_id'],\n",
      "    num_rows: 1034\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_normalize_spider_split(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Prepare lists to hold normalized data for Dataset.from_dict\n",
    "    questions = []\n",
    "    queries = [] # This will hold the SQL strings\n",
    "    db_ids = []\n",
    "\n",
    "    for item in data:\n",
    "        # Ensure 'query' is always a string.\n",
    "        # Spider's dev.json and train_spider.json typically have 'query' as a string.\n",
    "        # If it's a list (e.g., from train_others.json or other Spider variants),\n",
    "        # you might need to decide how to handle it (e.g., take the first one, join them).\n",
    "        # For standard Spider, 'query' should be a string.\n",
    "        sql_query = item['query']\n",
    "        if isinstance(sql_query, list):\n",
    "            # This case shouldn't happen for standard train_spider.json/dev.json\n",
    "            # but this handles the \"cannot mix list and non-list\" error.\n",
    "            # We'll join them, or take the first one. Let's take the first for simplicity.\n",
    "            sql_query = sql_query[0] if sql_query else \"\"\n",
    "            print(f\"Warning: Found list in 'query' field. Taking first element: {item['query']}\")\n",
    "        \n",
    "        # Ensure all fields are present (though Spider typically is clean)\n",
    "        if 'question' in item and 'db_id' in item and sql_query is not None:\n",
    "            questions.append(item['question'])\n",
    "            queries.append(sql_query)\n",
    "            db_ids.append(item['db_id'])\n",
    "        else:\n",
    "            print(f\"Warning: Skipping incomplete item: {item.get('question', 'N/A')}\")\n",
    "\n",
    "    # Create a dictionary suitable for datasets.Dataset.from_dict\n",
    "    return Dataset.from_dict({\n",
    "        'question': questions,\n",
    "        'query': queries,\n",
    "        'db_id': db_ids\n",
    "    })\n",
    "\n",
    "print(\"\\nLoading and normalizing Spider train split...\")\n",
    "train_spider_dataset = load_and_normalize_spider_split(os.path.join(DATASET_DIR, 'train_spider.json'))\n",
    "print(\"Loading and normalizing Spider validation split...\")\n",
    "validation_spider_dataset = load_and_normalize_spider_split(os.path.join(DATASET_DIR, 'dev.json'))\n",
    "\n",
    "\n",
    "# Now assign to the spider_dataset dictionary as expected by the next steps\n",
    "spider_dataset = {\n",
    "    'train': train_spider_dataset,\n",
    "    'validation': validation_spider_dataset\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\nSpider Dataset loaded and normalized successfully!\")\n",
    "print(spider_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8add1bab-cca7-4ae9-8c1f-fd5c469418ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Schema Representation Function ---\n",
    "def get_schema_representation(db_id, db_schemas_dict):\n",
    "    \"\"\"\n",
    "    Generates a textual representation of the database schema for a given db_id.\n",
    "    Includes table names, column names, and their types.\n",
    "    \"\"\"\n",
    "    schema = db_schemas_dict[db_id]\n",
    "    schema_parts = []\n",
    "    \n",
    "    for table_idx, table_name_original in enumerate(schema['table_names_original']):\n",
    "        schema_parts.append(f\"table {table_idx}: {table_name_original}\")\n",
    "        \n",
    "        table_cols = []\n",
    "        for col_idx, (col_table_idx, col_name_original) in enumerate(schema['column_names_original']):\n",
    "            if col_table_idx == table_idx:\n",
    "                col_type = schema['column_types'][col_idx]\n",
    "                table_cols.append(f\"column {col_idx}: {col_name_original} ({col_type})\")\n",
    "        \n",
    "        if table_cols:\n",
    "            schema_parts.append(\"  \" + \"; \".join(table_cols))\n",
    "            \n",
    "    return \" | \".join(schema_parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85044e3-f76e-4920-a3b4-69dbf3d4d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Preprocessing Function for T5 (Tokenization) ---\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Prepares input text for T5 model by combining schema and question,\n",
    "    and tokenizes both input and target SQL queries.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(examples['question'])):\n",
    "        question = examples['question'][i]\n",
    "        query = examples['query'][i]\n",
    "        db_id = examples['db_id'][i]\n",
    "\n",
    "        schema_text = get_schema_representation(db_id, db_schemas)\n",
    "\n",
    "        input_text = f\"generate sql: {schema_text} | question: {question}\"\n",
    "        \n",
    "        inputs.append(input_text)\n",
    "        targets.append(query)\n",
    "\n",
    "    model_inputs = TOKENIZER(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = TOKENIZER(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eeafe99-5bce-41a5-a87f-cdb19a9779ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying preprocessing to the dataset (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 7000/7000 [00:06<00:00, 1157.26 examples/s]\n",
      "Map: 100%|█████████████████████████| 1034/1034 [00:00<00:00, 1497.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nApplying preprocessing to the dataset (this may take a few minutes)...\")\n",
    "tokenized_train_dataset = spider_dataset['train'].map(preprocess_function, batched=True, remove_columns=['question', 'query', 'db_id'])\n",
    "tokenized_eval_dataset = spider_dataset['validation'].map(preprocess_function, batched=True, remove_columns=['question', 'query', 'db_id'])\n",
    "print(\"Dataset tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58705ef8-8934-426e-bee0-909530ca6d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 't5-small' loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. Load Model ---\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "print(f\"\\nModel '{MODEL_NAME}' loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190aa1c2-e2f2-48c5-98f4-732e2eca32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./spider_t5_results\", # Directory to save checkpoints and logs\n",
    "    num_train_epochs=5,               # Keep 10 epochs (or more if eval loss keeps decreasing)\n",
    "                                       # Adjust based on observation of eval_loss\n",
    "    \n",
    "    # --- ADJUSTED FOR 6GB VRAM ---\n",
    "    per_device_train_batch_size=2,     # Reduced from 4 to 2. This is often the max for 6GB with long sequences.\n",
    "    per_device_eval_batch_size=2,      # Reduced from 4 to 2 for consistency during evaluation.\n",
    "    gradient_accumulation_steps=2,     # Increased from 1 to 2. This makes the *effective* batch size 2 * 2 = 4.\n",
    "                                       # If you still get OOM, try:\n",
    "                                       # per_device_train_batch_size=1, gradient_accumulation_steps=4\n",
    "                                       # (This keeps effective batch size at 4 but uses even less peak VRAM)\n",
    "    \n",
    "    warmup_steps=200,                  # Keep 200\n",
    "    weight_decay=0.01,                 # Keep as is\n",
    "    learning_rate=5e-5,                # Keep 5e-5\n",
    "    logging_dir=\"./spider_t5_logs\",    # Directory for TensorBoard logs\n",
    "    logging_steps=50,                  # Log training metrics every 50 steps\n",
    "    eval_strategy=\"steps\",       # Evaluate model performance every 'eval_steps'\n",
    "    eval_steps=200,                    # Perform evaluation every 200 steps\n",
    "    save_strategy=\"steps\",             # Save a model checkpoint every 'save_steps'\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,       # After training, load the model with the best eval_loss\n",
    "    metric_for_best_model=\"eval_loss\", # Metric to monitor for saving the best model\n",
    "    report_to=\"none\",                  # Disable reporting to services like Weights & Biases if not using\n",
    "    fp16=True,                         # ABSOLUTELY Keep this enabled. It's crucial for 6GB VRAM.\n",
    "    # save_total_limit=3, # Keeps only the best 3 checkpoints (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0b75bda-7778-4da2-a3df-c4576431e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41766/2507042305.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Create Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=TOKENIZER, # Pass tokenizer here, it's used for data collation and generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e72263-5828-48b6-95a4-862c602d0c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on Spider dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8750' max='8750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8750/8750 1:09:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.254079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.136297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.113985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.103104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.096909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.093514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.095755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.089736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.104546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.093790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.086971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.087186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.090590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.083713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.082782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.091528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.083490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.081214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.087480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.088669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.085660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.084076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.080939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.081949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.083141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.083162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.083102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.088191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.088447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.088076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.087415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.088081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.079680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.080781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.083433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.086377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.090232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.088224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.085394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.086442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.087168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.087292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.087315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished successfully!\n",
      "\n",
      "Fine-tuned model and tokenizer saved to ./fine_tuned_t5_spider_sql_generator\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Train the Model ---\n",
    "print(\"\\nStarting training on Spider dataset...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training finished successfully!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nTraining interrupted due to RuntimeError: {e}\")\n",
    "    print(\"This often means Out-of-Memory (OOM). Try reducing 'per_device_train_batch_size' or 'max_length'.\")\n",
    "\n",
    "\n",
    "# --- 10. Save the fine-tuned model ---\n",
    "model_save_path = \"./fine_tuned_t5_spider_sql_generator\"\n",
    "model.save_pretrained(model_save_path)\n",
    "TOKENIZER.save_pretrained(model_save_path)\n",
    "print(f\"\\nFine-tuned model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2930f9b-6ee8-4aa7-9f4d-af563cb32322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
